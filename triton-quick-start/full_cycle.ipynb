{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828e30fa",
   "metadata": {},
   "source": [
    "# QuickStart по работе с Triton \n",
    "\n",
    "Данный ноутбук демонстрирует полный цикл обучения, конвертации модели и запуск инференса Triton.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Что такое NVIDIA Triton?\n",
    "Triton Inference Server оптимизирует вывод ИИ, позволяя командам развертывать, запускать и масштабировать обученные модели ИИ из любой среды в любой инфраструктуре на основе графического процессора или процессора. Это дает исследователям искусственного интеллекта и специалистам по данным свободу выбора правильной платформы для своих проектов, не влияя на производственное развертывание\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547befb0",
   "metadata": {},
   "source": [
    "## Установка зависимостей "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcfadf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0290b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, torch, datasets\n",
    "print(\"transformers\", transformers.__version__)\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"datasets\", datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18a244",
   "metadata": {},
   "source": [
    "## Набор данных\n",
    "\n",
    "В этом примере используется датасет [emotion](https://huggingface.co/datasets/emotion). Этот датасет содержит набор сообщений из Twitter и размечен на 6 эмоций sadness (0), joy (1), love (2), anger (3), fear (4), surprise (5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca3e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "dataset = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b75a81",
   "metadata": {},
   "source": [
    "## Предобработка \n",
    "\n",
    "Этот этап необходим для предобработки текстовых сообщений (конвертации текста в вектор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeed7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d1083",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cbabf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da044865",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478c24f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ab005",
   "metadata": {},
   "source": [
    "# Инференс"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712a6e4",
   "metadata": {},
   "source": [
    "Для удобства использования модели в инференсе, можно переименовать параметры с помощью словарей label2id и id2label. Это позволит при выводе результатов, видеть классы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535cb0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "label2id = {\n",
    "    \"sadness\": 0,\n",
    "    \"joy\": 1,\n",
    "    \"love\": 2,\n",
    "    \"anger\": 3,\n",
    "    \"fear\": 4,\n",
    "    \"surprise\": 5\n",
    "  }\n",
    "id2label = {\n",
    "    0: \"sadness\",\n",
    "    1: \"joy\",\n",
    "    2: \"love\",\n",
    "    3: \"anger\",\n",
    "    4: \"fear\",\n",
    "    5: \"surprise\"\n",
    "  }\n",
    "model_ckpt = \"./results/checkpoint-5000\"\n",
    "config = AutoConfig.from_pretrained(model_ckpt, label2id=label2id, id2label=id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./results/checkpoint-5000\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./results/checkpoint-5000\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04f3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am incredibly happy to start using Triton on ML-Space from SberCloud\"\n",
    "\n",
    "tensor = tokenizer(text, padding=\"max_length\",  truncation=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9acf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example output\", model(**tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5049380",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(**tensor).logits\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "del config\n",
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f695c",
   "metadata": {},
   "source": [
    "## Подготовка модели к инференсу на Triton\n",
    "\n",
    "\n",
    "Для инференса модели на Triton необходимо PyTorch модель перевести в TorchScript. Для этой конвертации неоходимо показать модели пример входного и выходного вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./results/checkpoint-5000\")\n",
    "\n",
    "tensors = tokenizer(text, padding=\"max_length\",  truncation=True, return_tensors='pt', max_length=512)\n",
    "example_inputs = tensors['input_ids'], tensors['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4896789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PyTorch_to_TorchScript(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyTorch_to_TorchScript, self).__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"./results/checkpoint-5000\")\n",
    "\n",
    "    def forward(self,data, attention_mask=None):\n",
    "        return self.model(data, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = PyTorch_to_TorchScript().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abcee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model = torch.jit.trace(pt_model, [tensors['input_ids'], tensors['attention_mask']], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe49742",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174975a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model(tensors['input_ids'], tensors['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4292f8",
   "metadata": {},
   "source": [
    "Перед сохранением модели необходимо создать каталог:\n",
    "\n",
    "```\n",
    "model_repository_path/\n",
    "|- <pytorch_model_name>/\n",
    "|  |- config.pbtxt\n",
    "|  |- 1/\n",
    "|     |- model.pt\n",
    "|\n",
    "```\n",
    "\n",
    "Где **pytorch_model_name** - название модели, **config.pbtxt** - конфигурация для Triton, **model.pt** - экспортированная модель. Структура каталогов будет выглядеть так:\n",
    "\n",
    "```\n",
    "triton_inf/\n",
    "|- / distil_bert_emotion\n",
    "|  |- config.pbtxt\n",
    "|  |- 1/\n",
    "|     |- model.pt\n",
    "|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0bfb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir Triton\n",
    "!mkdir Triton/Predictor\n",
    "!mkdir Triton/Predictor/distil_bert_emotion\n",
    "!mkdir Triton/Predictor/distil_bert_emotion/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model.save('./Triton/Predictor/distil_bert_emotion/1/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e5bd18",
   "metadata": {},
   "source": [
    "Теперь необходимо описать модель для Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d8870",
   "metadata": {},
   "source": [
    "Пример **config.pbtxt** \n",
    "```\n",
    "name: \"distil_bert_emotion\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [1, 512]\n",
    "  } ,\n",
    "{\n",
    "    name: \"input__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [1, 512]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1, 6]\n",
    "  }\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "        count: 1\n",
    "        kind: KIND_GPU\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "Где поле **name** - наименование модели,  **input** - описывает входной массив модели, **output** - описывает выходной массив. \n",
    "\n",
    "**input** указываются входные вектора. В этом примере у нас два входных вектора *input_ids* и *attention_mask* каждый имеет размерность `[1,512]` и тип данных `int32`. \n",
    "\n",
    "**output** указывает выходной вектор. В этом примере выходной вектор `[1,6]` и формат fp32\n",
    "\n",
    "Более подробно о написании **config.bptxt** можно ознакомиться в документации [Triton](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    " \n",
    "cat >> Triton/Predictor/distil_bert_emotion/config.pbtxt << EOF\n",
    "name: \"distil_bert_emotion\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "input [\n",
    " {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [1, 512]\n",
    "  } ,\n",
    "{\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [1, 512]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1, 6]\n",
    "  }\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "        count: 1\n",
    "        kind: KIND_GPU\n",
    "    }\n",
    "]\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1634657",
   "metadata": {},
   "source": [
    "## Transformer-скрипт"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f485c",
   "metadata": {},
   "source": [
    "Serving-скрипт отвечает за получение запроса, предобработку, отправку запроса в предиктор, постобработку предиктора.\n",
    "\n",
    "Для предобработки используется AutoTokenizer, ему необходимо указать откуда загрузить токенизатор.\n",
    "\n",
    "Для этого создадим директорию Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir Triton/Transformer\n",
    "!mkdir Triton/Transformer/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp results/checkpoint-5000/tokenizer.json Triton/Transformer/tokenizer\n",
    "!cp results/checkpoint-5000/tokenizer_config.json Triton/Transformer/tokenizer\n",
    "!cp results/checkpoint-5000/vocab.txt Triton/Transformer/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l Triton/Transformer/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Файл Transformer/kf_serving.py\n",
    "\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import kfserving\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "import logging\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "class BertTransformer(kfserving.KFModel):\n",
    "    def __init__(self, name: str, predictor_host: str):\n",
    "        super().__init__(name)\n",
    "        self.predictor_host = predictor_host\n",
    "        # токенайзер с сохранеными файлами\n",
    "        # из соображений безопастности доступа в интернет из контейнера деплоя нет\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('./tokenizer') \n",
    "        # наименование модели (из configb.pbtxt)\n",
    "        self.model_name = \"distil_bert_emotion\" \n",
    "        self.triton_client = None\n",
    "\n",
    "    def preprocess(self, inputs: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "            Препроцесинг входных данных \n",
    "        \"\"\"\n",
    "         # токенезируем входной запрос\n",
    "        tensors = self.tokenizer(inputs[\"instances\"][0], padding=\"max_length\",  truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "        return {\"input_ids\":tensors['input_ids'], \"attention_mask\":tensors['attention_mask']}\n",
    "\n",
    "    def predict(self, features: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "            Предикт     \n",
    "        \"\"\"\n",
    "        if not self.triton_client:\n",
    "            self.triton_client = httpclient.InferenceServerClient(\n",
    "                url=self.predictor_host, verbose=True)\n",
    "\n",
    "        input_ids = np.array(features['input_ids'], dtype=np.int32) # конвертируем вектор  в int32\n",
    "        attention_mask = np.array(features['attention_mask'], dtype=np.int32) # конвертируем вектор  в int32\n",
    "\n",
    "        input_ids = features[\"input_ids\"].reshape(1, 512) # преобразуем в [1,512]\n",
    "        attention_mask = features[\"attention_mask\"].reshape(1, 521)  # преобразуем в [1,512]\n",
    "\n",
    "        # Формируем запрос в тритон\n",
    "        inputs = [httpclient.InferInput('input_ids', [1, 512], \"INT32\"), \n",
    "                  httpclient.InferInput('attention_mask', [1, 512], \"INT32\")]  \n",
    "        # Заполняем запрос данными из numpy массива\n",
    "        inputs[0].set_data_from_numpy(input_ids) \n",
    "        inputs[1].set_data_from_numpy(attention_mask)\n",
    "\n",
    "        \n",
    "        # Указываем ожидаемый выходной результат сети\n",
    "        outputs = [httpclient.InferRequestedOutput('output__0', binary_data=False),] \n",
    "        result = self.triton_client.infer(self.model_name, inputs, outputs=outputs)\n",
    "        return result.get_response()\n",
    "\n",
    "    def postprocess(self, result: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "            Обработка результата сети\n",
    "        \"\"\"\n",
    "        logging.info(result)\n",
    "        prediction = result['outputs'][0]['data']\n",
    "\n",
    "        return {\"predictions\": prediction}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--http-port\", default=8080)\n",
    "    parser.add_argument(\"--predictor-host\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    x = re.compile('(kfserving-\\d+)').search(os.environ.get('HOSTNAME'))\n",
    "    name = \"kfserving-default\"\n",
    "    if x:\n",
    "        name = x[0]\n",
    "    model = BertTransformer(name, predictor_host=args.predictor_host)\n",
    "    kfserving.KFServer(workers=1, http_port=args.http_port).start([model])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba6677",
   "metadata": {},
   "source": [
    "Сформированный скрипт для сервинга модели необходимо сохранить по пути `Triton/Transformer/kf_serving.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ad90f",
   "metadata": {},
   "source": [
    "Для работы kf_serving.py скрипта необходимо добавить в установку используемые в нем зависимости. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c824251",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    " \n",
    "cat >> Triton/Transformer/requirements.txt << EOF\n",
    "tritonclient [all]\n",
    "transformers\n",
    "torch\n",
    "numpy\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb1bd2",
   "metadata": {},
   "source": [
    "Итоговая структура директории:\n",
    "```\n",
    " |-Triton\n",
    " | |-Transformer\n",
    " | | |-tokenizer\n",
    " | | | |-tokenizer.json\n",
    " | | | |-tokenizer_config.json\n",
    " | | | |-vocab.txt\n",
    " | | |-requirements.txt\n",
    " | | |-kf_serving.py\n",
    " | |-Predictor\n",
    " | | |-distil_bert_emotion\n",
    " | | | |-1\n",
    " | | | | |-model.pt\n",
    " | | | |-config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525423c3",
   "metadata": {},
   "source": [
    "## Создание образа"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c81451",
   "metadata": {},
   "source": [
    "Для сборки образа необходимо созданные папки **Transformer** и **Predictor** загрузить в бакет S3. Если бакет создан, то нужно перейти в раздел получения credentials. Для создания бакета S3 Data Catalog -> Обзор Хранилища -> Создать Бакет.\n",
    "\n",
    "\n",
    "<img src=\"img/data_storage.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "![data storage](img/storage.png)\n",
    "\n",
    "\n",
    "После создания бакета необходимо получить его credentials для подключения с помощью сторонних утилит и последующей загрузки файлов. \n",
    "\n",
    "\n",
    "<img src=\"img/get_cred.png\" alt=\"drawing\" width=\"500\"/>\n",
    "<img src=\"img/view_cred.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "\n",
    "После того как получили credentials необходимо скопировать: \n",
    "- S3 endpoint\n",
    "- S3 имя бакета\n",
    "- S3 access key ID\n",
    "- S3 security key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5525650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "S3_ACCESS_KEY_ID = \"ВАШ_S3_ACCESS_KEY_ID\"\n",
    "S3_SECRET_ACCESS_KEY_ID = \"ВАШ_S3_SECRET_ACCESS_KEY_ID\"\n",
    "BUCKET_NAME = \"ВАШ_BUCKET_NAME\"\n",
    "ENDPOINT_URL = \"ВАШ_ENDPOINT_URL\"\n",
    "\n",
    "def upload_files(bucket, path):\n",
    "    session = boto3.session.Session()\n",
    " \n",
    "    s3_client = session.client(\n",
    "        service_name='s3',\n",
    "        aws_access_key_id=S3_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=S3_SECRET_ACCESS_KEY_ID,\n",
    "        endpoint_url=ENDPOINT_URL\n",
    "    )\n",
    " \n",
    "    for subdir, dirs, files in tqdm(os.walk(path)):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            with open(full_path, 'rb') as data:\n",
    "                s3_client.put_object(Bucket = bucket, Key=full_path[len(path)+1:], Body=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127c56f",
   "metadata": {},
   "source": [
    "Загрузим каталоги из Triton в S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51216378",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_files(BUCKET_NAME, './Triton')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3d5ae",
   "metadata": {},
   "source": [
    "После загрузки можем приступить к сборке образа. Для сборки образа зайти в Deployment->Образы и нажмите \"Создать образ\"\n",
    "\n",
    "\n",
    "<img src=\"img/image.png\" alt=\"drawing\" width=\"900\"/>\n",
    "\n",
    "\n",
    "Первым образом, соберем \"Трансформер\".\n",
    "\n",
    "1. Тип образа  - Triton Server\n",
    "2. Тип контейнера - Трансформер\n",
    "3. Базовый образ - cr.msk.sbercloud.ru/aicloud-base-images/triton22.04-py3:0.0.32 \n",
    "4. Хранилище - тот S3 бакет в который загружали ранее \n",
    "5. Конфигурация\n",
    "    - Папка с моделью -  Transformer \n",
    "    - Файл Serving-script - kf_serving.py\n",
    "    - Файл Requirements - requirements.txt\n",
    "    \n",
    "<img src=\"img/image_build_transformer.png\" alt=\"\" width=\"900\"/>\n",
    "\n",
    "Вторым образом, соберем \"Предиктор\".\n",
    "\n",
    "1. Тип образа  - Triton Server\n",
    "2. Тип контейнера - Предиктор\n",
    "3. Базовый образ - cr.msk.sbercloud.ru/aicloud-base-images/triton22.04-py3:0.0.32 \n",
    "4. Хранилище - тот S3 бакет в который загружали ранее \n",
    "5. Конфигурация\n",
    "    - Папка с файлами конфигурации - папка с моделью. Пример - ИМЯ_БАКЕТА/Predictor\n",
    "\n",
    "\n",
    "<img src=\"img/image_build_predictor.png\" alt=\"\" width=\"900\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bd139",
   "metadata": {},
   "source": [
    "## Деплой\n",
    "\n",
    "\n",
    "Для деплоя модели зайдите в Deployment -> Деплои\n",
    "\n",
    "\n",
    "<img src=\"img/deploi.png\" alt=\"\" width=\"900\"/>\n",
    "\n",
    "Нажмите кнопку \"Создать деплой\". Укажите следующие настройки. \n",
    "\n",
    "1. Наименование - Название сборки (можно оставить пустым)\n",
    "2. Тип деплоя - Раздельный\n",
    "3. Ресурсы - указываем регион и тип конфигурации \n",
    "4. Указываем долю ресурсов от общей конфигурации для контейнера Transformer\n",
    "5. Выберите Docker-образ - указываете собранные Docker собранные ранее \n",
    "\n",
    "\n",
    "<img src=\"img/create_deploi.png\" alt=\"\" width=\"900\"/>\n",
    "\n",
    "\n",
    "\n",
    "После создания, появится карточка с созданным деплоем, со статусом **\"В очереди\"**. То есть данный деплой находиться на стадии ожидания выбранных ресурсов и как только ресурсы станут доступны, деплой передает в статус **\"Выполняется\"**\n",
    "\n",
    "Обратите внимание, что если минимальное количество Pods будет установлено в \"0\", то горячий деплой не будет запущен сразу. В таком случае при первом запросе, вы получите дополнительную задержку на поднятии деплоя. \n",
    "\n",
    "\n",
    "Открыв карточку с запущеным деплоем можно посмотреть и изменить текущую конфигурацию.\n",
    "\n",
    "<img src=\"img/image_triron.png\" alt=\"\" width=\"900\"/>\n",
    "\n",
    "Так же можно отправить тестовый запрос из вкладки \"Тест API\" и скопировать его в виде cURL \n",
    "\n",
    "<img src=\"img/image_example_requests.png\" alt=\"\" width=\"900\"/>\n",
    "\n",
    "Во вкладке \"Логи\" можно посмотреть текущее состояние деплоя Triton \n",
    "\n",
    "<img src=\"img/example_logs.png\" alt=\"\" width=\"900\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c374dc4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e740bb003c59f442552ef7e876805227e11287db79117ba91a0c5bdb1045b2fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
