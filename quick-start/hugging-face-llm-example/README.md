# Пример настройки и обучения языковой модели (LLM)

В этом примере рассмотрена работа с LLM (Large Language Model) методами LoRA и PEFT.
Используется библиотека transformers от Hugging Face.

В [файле Jupyter Notebook](llm-example.ipynb) показаны установка зависимостей, загрузка и настройка модели, обучение и генерация текста. Его можно использовать и модифицировать под собственные задачи.

В папке [job_launch](job_launch) находится пример запуска задачи обучения с использованием PyTorch DDP. В
В нем дообучается та же модель, что и в [llm-example.ipynb](llm-example.ipynb), дополнительно реализовано распределенное обучение.

В [пользовательской документации по LLM](https://cloud.ru/docs/aicloud/mlspace/concepts/tutorials/llm/tutorials__guides__llm_install.html) даны концептуальные понятия и подробные описания каждого этапа: предпосылки и объяснения работы методов и подходов.
