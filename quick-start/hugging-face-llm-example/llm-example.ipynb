{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb4fd29",
   "metadata": {},
   "source": [
    "# Дообучение большой языковой модели (LLM)\n",
    "\n",
    "Ноутбук можно запустить на платформе ML Space, [создав Jupyter Server](https://cloud.ru/docs/aicloud/mlspace/concepts/guides/guides__jupyter/environments__environments__jupyter-server__create-new-jupyter-server.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab0797-2979-4d2d-8a21-833ef7abec29",
   "metadata": {},
   "source": [
    "## 1. Установка зависимостей и импорт библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863621d6",
   "metadata": {},
   "source": [
    "Перед началом работы установим необходимые пакеты для обработки данных и обучения моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f72de5f-7529-4d8f-8b03-0fd2eb44b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqqq pip\n",
    "!pip install -qqq bitsandbytes torch transformers peft \\\n",
    "    accelerate datasets loralib==0.1.1 einops==0.6.1 scipy sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99cfe530-d1dd-45a9-a3ce-d397e0be06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40ad53-1030-4182-bbf7-0a7450e148d1",
   "metadata": {},
   "source": [
    "## 2. Загрузка базовой модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffb595",
   "metadata": {},
   "source": [
    "Загрузим подходящую предварительно обученную модель. В этом примере выбрана \"Intel/neural-chat-7b-v3-1\". Можно выбрать другую, заменив значение переменной `MODEL_NAME`.\n",
    "\n",
    "Используются следующие параметры:\n",
    "\n",
    "- `load_in_4bit` — загрузка модели в 4-битном формате с уменьшением ее размера в памяти;\n",
    "\n",
    "- `bnb_4bit_use_double_quant` — двойная квантизация для дополнительного уменьшения размера модели;\n",
    "\n",
    "- `bnb_4bit_quant_type` — тип квантизации (\"nf4\"), который определяет, как модель будет сжиматься;\n",
    "\n",
    "- `bnb_4bit_compute_dtype` — тип данных для вычислений (torch.bfloat16), который позволяет уменьшить потребление памяти и ускорить вычисления, сохраняя при этом точность.\n",
    "\n",
    "Сжимаем модель с помощью библиотеки `BitsAndBytes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c714cb15-6ac9-452a-a21f-194ac174922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Intel/neural-chat-7b-v3-1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe498b5-2e23-4854-ab25-8aad55d874ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ea4fb377ad428d8907df59fd5a7cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16199a-191b-47b8-9c17-af43182d5a2b",
   "metadata": {},
   "source": [
    "## 3. Добавление LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd92c41e",
   "metadata": {},
   "source": [
    "Реализуется метод LoRa, который позволяет изменять только небольшую часть параметров модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9348bb-5800-47ea-aef8-433bd83abaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "  \"\"\"\n",
    "  Calculates and displays the total number of model parameters and the number of trainable parameters.\n",
    "  \"\"\"\n",
    "  trainable_params = 0\n",
    "  all_param = 0\n",
    "  for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "      trainable_params += param.numel()\n",
    "  print(\n",
    "      f\"trainable params: {trainable_params} || all params: {all_param} || trainables%: {100 * trainable_params / all_param}\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5070dda9-0f61-4475-8090-73d3967e764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d873a7-3802-4ff5-8deb-fb8db9016b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4defb",
   "metadata": {},
   "source": [
    "Создадим конфигурация для LoRA (Low-Rank Adaptation), с помощью которой можно адаптировать только небольшую часть весов модели. \n",
    "\n",
    "Конфигурация имеет следующие параметры:\n",
    "\n",
    "- `r` и `lora_alpha` — параметры, контролирующие размер и мощность адаптации;\n",
    "\n",
    "- `target_modules` — список модулей модели, к которым будет применена адаптация LoRA;\n",
    "\n",
    "- `lora_dropout` — применение dropout к адаптированным весам;\n",
    "\n",
    "- `bias` — настройка использования смещения в адаптации;\n",
    "\n",
    "- `task_type` — тип задачи, для которой настраивается модель, в нашем случае это генерация текста.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23fd1331-9ad8-498e-882d-8129d38eea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    #target_modules=[\"query_key_value\"],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9660c929-58f3-4519-97c6-5dd42c0cca8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6815744 || all params: 3758886912 || trainables%: 0.18132346515244138\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f93204-5e45-4937-aad1-576f08b5e3f0",
   "metadata": {},
   "source": [
    "## 4. Дообучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e8f1ed",
   "metadata": {},
   "source": [
    "Проверим, как модель генерирует ответы на основе заданного запроса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e24a44d-311d-4411-85e7-6db31ff54312",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<human>: midjourney prompt for a girl sit on the mountain\n",
    "<assistant>:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e0264",
   "metadata": {},
   "source": [
    "Зададим параметры для генерации текста:\n",
    "- `max_new_tokens` — максимальное количество новых токенов, которые модель может сгенерировать;\n",
    "\n",
    "- `temperature` — степень случайности в выборе слов, где меньшее значение приводит к более предсказуемому тексту;\n",
    "\n",
    "- `top_p` — вероятностный порог выбора слов; рассматриваться будут только слова с вероятностью выше этого порога;\n",
    "\n",
    "- `num_return_sequences` — количество возвращаемых последовательностей;\n",
    "\n",
    "- `pad_token_id` и `eos_token_id` — идентификаторы токенов заполнения и окончания предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f645ce1b-3b78-4d83-8ae9-40db8985b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88136e3",
   "metadata": {},
   "source": [
    "Преобразуем запрос в формат, пригодный для модели, с помощью токенизатора и переместим данные на GPU.\n",
    "`return_tensors=\"pt\"` указывает на то, что возвращаемые тензоры должны быть в формате PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4201a61-10c2-4abe-9740-d6d9fd8f04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d253b20a-8c52-4326-9300-75c0cd1f49ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/llm_trainer/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/.mlspace/envs/llm_trainer/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.82 s, sys: 232 ms, total: 4.06 s\n",
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65482f88-d384-4164-984f-cb702ebbe141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: midjourney prompt for a girl sit on the mountain\n",
      "<assistant>: A young girl, dressed in a warm, cozy outfit, sits on a large boulder overlooking a vast, snow-capped mountain range. The sun is setting behind her, casting a golden glow on her face and the surrounding landscape. She gazes into the distance, lost in thought, as the cool breeze gently ruffles her hair.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc6d22-7428-4c40-8343-ce01a35120c2",
   "metadata": {},
   "source": [
    "## 5. Загрузка набора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a2e0b5-bb65-4405-ac7a-f48bf51d9827",
   "metadata": {},
   "source": [
    "Текстовый запрос к модели генерируется, обрабатывается и передается в модель. \n",
    "Для обучения использован датасет [Mid Journey Prompts от Hugging Face](https://huggingface.co/datasets/bittu9988/mid_journey_prompts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "835ee6bf-b259-45d5-94ab-89587d1ce8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"bittu9988/mid_journey_prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e687291a-61fb-412a-b8c4-350dc48f4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "  return f\"\"\"\n",
    "<human>: {data_point[\"User\"]}\n",
    "<assistant>: {data_point[\"Prompt\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "  full_prompt = generate_prompt(data_point)\n",
    "  tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "  return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f0762b8-49c8-4ea4-99b7-3c2d8fb47f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28ea6eba410418796c4c4b0769042fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/289 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb10ca9-bf6e-47f6-99a1-8a6e17658d9b",
   "metadata": {},
   "source": [
    "## 6. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53569d",
   "metadata": {},
   "source": [
    "Задаются параметры модели, и начинается ее дообучение."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ce9907",
   "metadata": {},
   "source": [
    "Зададим переменные окружения для использования MLflow. Подробнее — в [документации Hugging Face](https://huggingface.co/docs/transformers/v4.43.3/en/main_classes/callback#transformers.integrations.MLflowCallback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dde4f29f-648f-4328-ba2c-f7498097a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = \"trainer-llm\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"/home/jovyan/mlruns\"\n",
    "os.environ[\"MLFLOW_FLATTEN_PARAMS\"] = \"1\"\n",
    "os.environ[\"HF_MLFLOW_LOG_ARTIFACTS\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a40a2e",
   "metadata": {},
   "source": [
    "Создадим объект `TrainingArguments`, который содержит различные параметры для обучения:\n",
    "\n",
    "- `per_device_train_batch_size` — размер батча обучения на каждом устройстве;\n",
    "\n",
    "- `gradient_accumulation_steps` — количество шагов накопления градиента перед их обратным распространением;\n",
    "\n",
    "- `num_train_epochs` — количество эпох обучения;\n",
    "\n",
    "- `learning_rate` — скорость обучения;\n",
    "\n",
    "- `fp16` — использование 16-битной точности с плавающей запятой для ускорения обучения и снижения потребления памяти;\n",
    "\n",
    "- `save_total_limit` — максимальное количество сохраняемых чекпоинтов;\n",
    "\n",
    "- `logging_steps` — частота логирования;\n",
    "\n",
    "- `output_dir` — директория для сохранения результатов обучения;\n",
    "\n",
    "- `optim` — оптимизатор, здесь используется 8-битная версия AdamW;\n",
    "\n",
    "- `lr_scheduler_type` — тип планировщика скорости обучения;\n",
    "\n",
    "- `warmup_ratio` — доля общего числа шагов обучения, в течение которых скорость обучения линейно увеличивается до заданной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e2fd271-5c95-4c0a-aad7-0c30a114e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "      per_device_train_batch_size=1,\n",
    "      gradient_accumulation_steps=8,\n",
    "      num_train_epochs=4,\n",
    "      learning_rate=2e-4,\n",
    "      fp16=True,\n",
    "      save_total_limit=3,\n",
    "      logging_steps=1,\n",
    "      output_dir=\"experiments\",\n",
    "      optim=\"paged_adamw_8bit\",\n",
    "      lr_scheduler_type=\"cosine\",\n",
    "      warmup_ratio=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c9655",
   "metadata": {},
   "source": [
    "`Trainer` отвечает за процесс обучения модели и имеет следующие параметры:\n",
    "\n",
    "- `model` — модель, которая будет обучаться;\n",
    "\n",
    "- `train_dataset` — набор данных для обучения;\n",
    "\n",
    "- `args` — аргументы обучения, определенные выше;\n",
    "\n",
    "- `data_collator` — объект, который формирует батчи из данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0858d575-10dc-4e55-95e3-afad8b83b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1555548f",
   "metadata": {},
   "source": [
    "Эта настройка отключает кеширование в модели, что может быть полезно для экономии памяти во время обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35c1e159-3fbf-40ec-b598-bf26c8c09d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2581c",
   "metadata": {},
   "source": [
    "Запустим процесс обучения с помощью метода `train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca64f0e9-8435-462a-b0c9-c63a311d90e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/31 17:45:54 INFO mlflow.tracking.fluent: Experiment with name 'trainer-llm' does not exist. Creating a new experiment.\n",
      "/home/jovyan/.mlspace/envs/llm_trainer/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='144' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [144/144 04:57, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.928300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.311100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.203400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.776600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.670800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.597100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.576900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.465800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.812800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.911500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.917200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.822800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.396300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.422300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.261300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.492300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.160400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.539500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.663100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.602800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.583600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.320800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.378900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.300800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.661900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.963500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.620300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.923200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.516600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.429200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.871500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.557900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.278300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.276900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.816600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.826300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.789300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.563200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.571300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.674800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.539100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.732600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.802700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.818600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.098300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.612600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.543100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.819700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.523700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.652000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.270400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.604500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.657800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.648700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.538700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.483300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.480300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.644100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.555500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.552700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.591200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.309300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.415400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.863800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.815200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.279900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.746300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.418300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.314200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.832900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.510100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.737100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.491900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.229500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.597600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.346500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.857800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.469900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.291800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.350700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.418300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af7daeb325e4e819c1c02244e2adbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/llm_trainer/lib/python3.11/site-packages/_distutils_hack/__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "2024/07/31 17:50:52 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=144, training_loss=1.2027036458667781, metrics={'train_runtime': 298.6398, 'train_samples_per_second': 3.871, 'train_steps_per_second': 0.482, 'total_flos': 3939821811425280.0, 'train_loss': 1.2027036458667781, 'epoch': 3.986159169550173})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101ac7f-fcfe-48f1-b342-bb92abe84008",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4af65ed-6ae1-4505-81f0-4ebd558660b9",
   "metadata": {},
   "source": [
    "## 7. Сохранение модели и инференс"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b26bf",
   "metadata": {},
   "source": [
    "Сохраним в папку `trained-model` текущее состояние модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ed5a269-8179-44d7-84d3-7a9015c3483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"trained-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a24689",
   "metadata": {},
   "source": [
    "Загрузим конфигурацию PEFT (Parameter-Efficient Fine-Tuning) из сохраненной модели. Далее создадим новый экземпляр модели с помощью этой конфигурации. \n",
    "\n",
    "Параметры `return_dict=True`, `quantization_config=bnb_config`, `device_map=\"auto\"` и `trust_remote_code=True` настраивают поведение модели, включая формат возвращаемых данных, настройки квантизации, автоматическое распределение по устройствам и доверие к исполняемому коду.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8662241-dcc6-486a-981d-0a1ff46bec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "config = PeftConfig.from_pretrained('./trained-model')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16883efc-1adf-4dc4-9ec5-8c3a677581fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcc712fa-e63d-44d0-b7cd-70ca4e007115",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, './trained-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e59f628-89e8-4220-998d-4f06a784145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc02962f",
   "metadata": {},
   "source": [
    "Создадим и токенизируем тестовый запрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ca3f86c-0667-4e58-8ed7-901f1c93e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "<human>: midjourney prompt for \n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d88bb",
   "metadata": {},
   "source": [
    "В блоке ``with torch.no_grad()`` отключается вычисление градиентов для экономии ресурсов. \n",
    "\n",
    "Метод model.generate используется для генерации текста на основе входных данных (`input_ids`, `attention_mask`) и предварительно настроенных параметров генерации (`generation_config`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10956a75-eb98-4bbd-a440-30408d0f49c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/user/conda/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 68 ms, total: 10.6 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with torch.no_grad():\n",
    "  model.config.use_cache = False\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6d0653",
   "metadata": {},
   "source": [
    "Выведем сгенерированный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7985eae-0526-4f7d-a2ec-d8a0e7e75217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <human>: midjourney prompt for \n",
      "<assistant>: 8k octane render of a realistic furry creature in a forest environment with Pixar render, pencil test, pixar style, character studio, character render, high detail, hyper-realistic, photorealistic, cinematic, 8k, octane render, environment, nature, forest, greens, plants, high detail, 8k, octane render, cinematic, 8k, high detail, hyper-realistic, photorealistic, pixar style, character studio, character render, environment, nature, forest, greens, plants, high detail, 8k, octane render, cinematic, 8k, high detail, hyper-realistic, photorealistic, pixar style, character studio, character render, environment, nature, forest, greens, plants, high detail, 8k, octane render, cinematic, 8k,\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-llm_trainer]",
   "language": "python",
   "name": "conda-env-.mlspace-llm_trainer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
