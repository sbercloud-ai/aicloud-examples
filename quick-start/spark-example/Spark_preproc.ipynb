{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Подключение библиотеки pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Создание SparkSession и SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Конфигурирование доступа к хранилищу S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Укажите параметры доступа к своему хранилищу S3: Endpoint, Acess key, Secret key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"https://your_endpoint_name\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"your_access_key\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"your_secret_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Загрузка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим схему данных датасета, и создадим Spark Dataframe, посредством которого мы будем работать с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField('num', T.IntegerType(), True),\n",
    "    T.StructField('sensor_id', T.IntegerType(), True),\n",
    "    T.StructField('location', T.IntegerType(), True),\n",
    "    T.StructField('lat', T.DoubleType(), True),\n",
    "    T.StructField('lon', T.DoubleType(), True),\n",
    "    T.StructField('timestamp', T.TimestampType(), True),\n",
    "    T.StructField('pressure', T.DoubleType(), True),\n",
    "    T.StructField('temperature', T.DoubleType(), True),\n",
    "    T.StructField('humidity', T.DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве данных используем CSV-файл объемом 7.8 GB, собранный из данных, расположенных на https://www.kaggle.com/hmavrodiev/sofia-air-quality-dataset. Данные содержат записи с датчиков погоды."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Укажите путь до датасета на вашем бакете S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 's3a://your_bucket_name/path/dataset.csv'\n",
    "\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(schema) \\\n",
    "    .load(path)\n",
    "    \n",
    "df = df.drop('num').withColumn('hour', F.hour(F.col('timestamp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor_id: integer (nullable = true)\n",
      " |-- location: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Темерь мы можем увидеть Spark Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+------------------+-------------------+--------+-----------+--------+----+\n",
      "|sensor_id|location|               lat|               lon|          timestamp|pressure|temperature|humidity|hour|\n",
      "+---------+--------+------------------+------------------+-------------------+--------+-----------+--------+----+\n",
      "|     2266|    1140|            42.738|            23.272|2017-07-01 00:00:07|95270.27|      23.46|   62.48|   0|\n",
      "|     2292|    1154|42.663000000000004|23.273000000000003|2017-07-01 00:00:08|94355.83|      23.06|   59.46|   0|\n",
      "|     3096|    1558|              42.7|             23.36|2017-07-01 00:00:10|95155.81|      26.53|   44.38|   0|\n",
      "|     3428|    1727|42.623999999999995|            23.406|2017-07-01 00:00:12|94679.57|      28.34|   38.28|   0|\n",
      "|     3472|    1750|            42.669|            23.318|2017-07-01 00:00:13|94327.88|      26.31|   46.37|   0|\n",
      "|     1952|     976|42.708999999999996|23.398000000000003|2017-07-01 00:00:13|95314.52|      22.66|   56.55|   0|\n",
      "|     1846|     923|             42.64|             23.31|2017-07-01 00:00:15|93616.77|      23.87|   50.76|   0|\n",
      "|     3512|    1770|            42.683|            23.335|2017-07-01 00:00:24|94962.39|      24.92|   55.53|   0|\n",
      "|     2228|    1120|42.693000000000005|23.333000000000002|2017-07-01 00:00:28|94982.91|      26.29|    45.7|   0|\n",
      "|     3438|    1732|            42.738|23.293000000000003|2017-07-01 00:00:37|95099.81|      24.62|   57.97|   0|\n",
      "+---------+--------+------------------+------------------+-------------------+--------+-----------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество строк до препроцессинга (это может занять время):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97288452"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Препроцессинг данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы хотим использовать SQL-синтаксис для запросов Spark, мы должны зарегистрировать временное представление для данных (ограниченное в рамках вашей сессии Spark). После этого мы сможем обращаться к нему по имени:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('weather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Команда ниже запускает типичное задание Spark и собирает результаты на Spark Driver. Запрос выбирает данные за дневные периоы, группирует по расположению, и подсчитывает некоторые статистики для каждого расположения (это может занять время):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql('''\n",
    "    select \n",
    "        location as location_id, \n",
    "        count(1) as data_num, \n",
    "        avg(pressure) as mean_pressure,\n",
    "        avg(humidity) as mean_humidity,\n",
    "        max(temperature) as max_temp\n",
    "    from weather \n",
    "    where hour > 9 and hour < 20 \n",
    "    group by location\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Препроцессинг был успешно осуществлён, размер датасета уменьшился с 97 288 452 до 485 строк. Теперь мы можем, к примеру, загрузить данные в датафрейм Pandas, чтобы продолжить работу с ними на Spark Driver или в любом другом расположении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>data_num</th>\n",
       "      <th>mean_pressure</th>\n",
       "      <th>mean_humidity</th>\n",
       "      <th>max_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1025</td>\n",
       "      <td>175218</td>\n",
       "      <td>95410.270136</td>\n",
       "      <td>60.886515</td>\n",
       "      <td>41.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2580</td>\n",
       "      <td>97453</td>\n",
       "      <td>93361.977317</td>\n",
       "      <td>71.473486</td>\n",
       "      <td>31.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12006</td>\n",
       "      <td>29580</td>\n",
       "      <td>94663.426777</td>\n",
       "      <td>48.635029</td>\n",
       "      <td>47.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1139</td>\n",
       "      <td>158230</td>\n",
       "      <td>94872.799095</td>\n",
       "      <td>51.396233</td>\n",
       "      <td>43.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3488</td>\n",
       "      <td>141273</td>\n",
       "      <td>93170.706796</td>\n",
       "      <td>61.865252</td>\n",
       "      <td>38.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>3050</td>\n",
       "      <td>107306</td>\n",
       "      <td>89184.191561</td>\n",
       "      <td>63.792659</td>\n",
       "      <td>36.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>3245</td>\n",
       "      <td>123686</td>\n",
       "      <td>94246.646024</td>\n",
       "      <td>65.139249</td>\n",
       "      <td>36.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>3376</td>\n",
       "      <td>143159</td>\n",
       "      <td>94642.903072</td>\n",
       "      <td>50.533826</td>\n",
       "      <td>36.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>5836</td>\n",
       "      <td>26272</td>\n",
       "      <td>95171.041646</td>\n",
       "      <td>52.475399</td>\n",
       "      <td>33.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>12285</td>\n",
       "      <td>267</td>\n",
       "      <td>74396.942921</td>\n",
       "      <td>88.398464</td>\n",
       "      <td>26.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>485 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     location_id  data_num  mean_pressure  mean_humidity  max_temp\n",
       "0           1025    175218   95410.270136      60.886515     41.91\n",
       "1           2580     97453   93361.977317      71.473486     31.33\n",
       "2          12006     29580   94663.426777      48.635029     47.89\n",
       "3           1139    158230   94872.799095      51.396233     43.61\n",
       "4           3488    141273   93170.706796      61.865252     38.41\n",
       "..           ...       ...            ...            ...       ...\n",
       "480         3050    107306   89184.191561      63.792659     36.01\n",
       "481         3245    123686   94246.646024      65.139249     36.09\n",
       "482         3376    143159   94642.903072      50.533826     36.56\n",
       "483         5836     26272   95171.041646      52.475399     33.29\n",
       "484        12285       267   74396.942921      88.398464     26.36\n",
       "\n",
       "[485 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame.from_records(map(lambda x: x.asDict(), result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
