{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7045c9-7d78-43e2-a080-b854843bb962",
   "metadata": {},
   "source": [
    "# Обучение модели с использованием PyTorch Elastic Learning\n",
    "\n",
    "PyTorch Elastic Learning при обучении моделей позволяет масштабировать вычислительные ресурсы в зависимости от их доступности и повышает отказоустойчивость. Подробнее об использовании PyTorch Elastic Learning – в [документации ML Space](https://cloud.ru/docs/aicloud/mlspace/concepts/guides/guides__mt/environments__model-training__training-with-elastic-learning-example.html#pytorch-elastic-learning) и в [документации PyTorch](https://pytorch.org/docs/stable/elastic/run.html).\n",
    "\n",
    "Чтобы не потерять результаты обучения при перезапуске задачи, в коде реализуют [сохранение чекпоинтов и запуск обучения с последнего чекпоинта](https://cloud.ru/docs/aicloud/mlspace/concepts/guides/guides__mt/environments__model-training__save-intermidiate-results.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72284e2-f0d5-45fe-9c1b-74373d50f297",
   "metadata": {},
   "source": [
    "## 1. Подготовка\n",
    "\n",
    "Импортируем библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a776b097-47b5-4090-b2dc-e03717ebe808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import client_lib\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85a7b1a-92ca-401f-9590-fffabde591a1",
   "metadata": {},
   "source": [
    "Устанавливаем переменные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b618a1e-32ca-4f11-b325-677d1d9baa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: /home/jovyan/aicloud-examples/pytorch-elastic-example\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = pathlib.Path().absolute()\n",
    "print(f'Working dir: {BASE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e49ce-d5a0-44d8-9fa4-5e539a15a388",
   "metadata": {},
   "source": [
    "# 2. Запуск задачи обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d12e0-acbd-4be4-8a18-b6bf878a3f4c",
   "metadata": {},
   "source": [
    "Запускать задачу в регионе будем с помощью класса `client_lib.Job()`. \n",
    "\n",
    "См. [Все способы запустить задачу обучения](https://cloud.ru/docs/aicloud/mlspace/concepts/guides/guides__mt/environments__model-training__training-process.html)\n",
    "\n",
    "\n",
    "Для запуска укажем следующие параметры:\n",
    "- `script`– путь к запускаемому скрипту;\n",
    "- `base_image` – базовый образ, в котором будет исполняться скрипт обучения модели;\n",
    "- `instance_type` – конфигурация вычислительных ресурсов, используемых для решения задач;\n",
    "- `type` - тип распределённых вычислений;\n",
    "- `n_workers` — количество рабочих узлов;\n",
    "- `elastic_min_workers` — минимальное количество рабочих узлов;\n",
    "- `elastic_max_workers` — максимальное количество рабочих узлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf35970-3386-4bee-8238-f4a43ae2f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"SR006\" # Region\n",
    "INSTANCE_TYPE = \"a100plus.1gpu.80vG.12C.96G\" # Instance_type\n",
    "N_WORKERS = 3\n",
    "BASE_IMAGE = \"cr.ai.cloud.ru/aicloud-base-images/cuda12.1-torch2-py39:0.0.36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aee1401-dc82-4e9c-8fe7-1cd799ee9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run = client_lib.Job(\n",
    "    base_image=BASE_IMAGE,\n",
    "    script=f'{BASE_DIR}/train_ddp_elastic_example-torch2.py',\n",
    "    region=f'{REGION}',\n",
    "    instance_type=f'{INSTANCE_TYPE}',\n",
    "    flags={\"epochs\": 10},\n",
    "    type=\"pytorch_elastic\",\n",
    "    n_workers=N_WORKERS,\n",
    "    elastic_min_workers=1,\n",
    "    elastic_max_workers=3,\n",
    "    checkpoint_dir=f\"{BASE_DIR}/logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b9a012-b0fa-4d13-b03d-6995bee348a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job \"lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811\" created.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_run.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b38d9fa3-6b5d-4432-906e-086f8c41a402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job status=Completed'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_run.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5415742b-671e-42a6-851b-28d5337c7d00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-20T10:20:03Z master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO] Starting elastic_operator with launch configs:\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO]   entrypoint       : /home/jovyan/aicloud-examples/pytorch-elastic-example/train_ddp_elastic_example-torch2.py\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO]   min_nodes        : 1\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO]   max_nodes        : 3\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO]   nproc_per_node   : 1\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO]   run_id           : elastic\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO]   rdzv_backend     : c10d\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO]   rdzv_endpoint    : localhost:29500\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO]   rdzv_configs     : {'join_timeout': '1200', 'last_call_timeout': '90', 'read_timeout': '90', 'timeout': 900}\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO]   max_restarts     : 5\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO]   monitor_interval : 2.0\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO]   log_dir          : None\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO]   metrics_cfg      : {}\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,266] torch.distributed.launcher.api: [INFO] \n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,268] torch.distributed.elastic.agent.server.local_elastic_agent: [INFO] log directory set to: /tmp/torchelastic_jahdf76f/elastic__ip7t3ta\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,268] torch.distributed.elastic.agent.server.api: [INFO] [] starting workers for entrypoint: python\n",
      "2024-09-20T10:20:03Z [2024-09-20 13:20:03,269] torch.distributed.elastic.agent.server.api: [INFO] [] Rendezvous'ing worker group\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,314] torch.distributed.elastic.agent.server.api: [INFO] [] Rendezvous complete for workers. Result:\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,314] torch.distributed.elastic.agent.server.api: [INFO]   restart_count=0\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,314] torch.distributed.elastic.agent.server.api: [INFO]   master_addr=lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0.lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811.ai0001011-05040.svc.cluster.local\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,314] torch.distributed.elastic.agent.server.api: [INFO]   master_port=40297\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,314] torch.distributed.elastic.agent.server.api: [INFO]   group_rank=0\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,314] torch.distributed.elastic.agent.server.api: [INFO]   group_world_size=3\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,314] torch.distributed.elastic.agent.server.api: [INFO]   local_ranks=[0]\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,314] torch.distributed.elastic.agent.server.api: [INFO]   role_ranks=[0]\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,314] torch.distributed.elastic.agent.server.api: [INFO]   global_ranks=[0]\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,314] torch.distributed.elastic.agent.server.api: [INFO]   role_world_sizes=[3]\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,314] torch.distributed.elastic.agent.server.api: [INFO]   global_world_sizes=[3]\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,314] torch.distributed.elastic.agent.server.api: [INFO] \n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,315] torch.distributed.elastic.agent.server.api: [INFO] [] Starting worker group\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,315] torch.distributed.elastic.agent.server.local_elastic_agent: [INFO] Starting a FileTimerServer with /tmp/watchdog_timer_5c72add0-1e0b-474c-a084-b974c26fda1d ...\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,315] torch.distributed.elastic.agent.server.local_elastic_agent: [INFO] FileTimerServer started\n",
      "2024-09-20T10:20:10Z [2024-09-20 13:20:10,316] torch.distributed.elastic.multiprocessing: [INFO] Setting worker0 reply file to: /tmp/torchelastic_jahdf76f/elastic__ip7t3ta/attempt_0/0/error.json\n",
      "2024-09-20T10:20:12Z /home/user/conda/lib/python3.9/site-packages/pydantic/_internal/_fields.py:128: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "2024-09-20T10:20:12Z \n",
      "2024-09-20T10:20:12Z You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "2024-09-20T10:20:12Z   warnings.warn(\n",
      "2024-09-20T10:20:12Z /home/user/conda/lib/python3.9/site-packages/pydantic/_internal/_config.py:317: UserWarning: Valid config keys have changed in V2:\n",
      "2024-09-20T10:20:12Z * 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "2024-09-20T10:20:12Z   warnings.warn(message, UserWarning)\n",
      "2024-09-20T10:20:12Z Working dir: /home/jovyan/aicloud-examples/pytorch-elastic-example\n",
      "2024-09-20T10:20:12Z local rank = 0, rank = 0\n",
      "2024-09-20T10:20:12Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:74 [0] NCCL INFO Bootstrap : Using eth0:10.224.13.62<0>\n",
      "2024-09-20T10:20:12Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:74 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v6 symbol.\n",
      "2024-09-20T10:20:12Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:74 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin (v5)\n",
      "2024-09-20T10:20:12Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:74 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\n",
      "2024-09-20T10:20:12Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:74 [0] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v5)\n",
      "2024-09-20T10:20:12Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:74 [0] NCCL INFO cudaDriverVersion 12020\n",
      "2024-09-20T10:20:12Z NCCL version 2.18.5+cuda12.1\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO P2P plugin IBext\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB eth0:10.224.13.62<0>\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Using network IBext\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO comm 0x85988f0 rank 0 nranks 3 cudaDev 0 nvmlDev 0 busId 9c000 commId 0xda84ceb4af45b162 - Init START\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Setting affinity for GPU 0 to aaaaaaaa,aaaaaaaa,aaaaaaaa,aaaaaaaa\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 00/04 :    0   1   2\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 01/04 :    0   1   2\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 02/04 :    0   1   2\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 03/04 :    0   1   2\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Trees [0] 2/-1/-1->0->-1 [1] 2/-1/-1->0->-1 [2] 2/-1/-1->0->1 [3] 2/-1/-1->0->1\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 02/0 : 2[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 03/0 : 2[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Connected all rings\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 00/0 : 0[0] -> 2[0] [send] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 01/0 : 0[0] -> 2[0] [send] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 02/0 : 0[0] -> 2[0] [send] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Channel 03/0 : 0[0] -> 2[0] [send] via NET/IBext/4/GDRDMA\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO Connected all trees\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 512 | 512\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer\n",
      "2024-09-20T10:20:13Z lm-mpi-job-05c11349-feca-4ded-903f-f961c5eb3811-mpimaster-0:74:208 [0] NCCL INFO comm 0x85988f0 rank 0 nranks 3 cudaDev 0 nvmlDev 0 busId 9c000 commId 0xda84ceb4af45b162 - Init COMPLETE\n",
      "2024-09-20T10:20:13Z Start train with num epoch = 10\n",
      "2024-09-20T10:20:14Z Epoch 1\n",
      "2024-09-20T10:20:14Z train loss = 20.500926971435547\n",
      "2024-09-20T10:20:14Z train loss = 0.6131929755210876\n",
      "2024-09-20T10:20:14Z train loss = 0.6751250624656677\n",
      "2024-09-20T10:20:14Z train loss = 0.45725858211517334\n",
      "2024-09-20T10:20:14Z \n",
      "2024-09-20T10:20:14Z Test set: Average loss: 0.1963, Accuracy: 93.8%\n",
      "2024-09-20T10:20:14Z \n",
      "2024-09-20T10:20:14Z Epoch 2\n",
      "2024-09-20T10:20:14Z train loss = 0.43206459283828735\n",
      "2024-09-20T10:20:15Z train loss = 0.16901256144046783\n",
      "2024-09-20T10:20:15Z train loss = 0.5222336053848267\n",
      "2024-09-20T10:20:15Z train loss = 0.5782862305641174\n",
      "2024-09-20T10:20:15Z train loss = 0.30960869789123535\n",
      "2024-09-20T10:20:15Z \n",
      "2024-09-20T10:20:15Z Test set: Average loss: 0.1107, Accuracy: 96.3%\n",
      "2024-09-20T10:20:15Z \n",
      "2024-09-20T10:20:15Z Epoch 3\n",
      "2024-09-20T10:20:15Z train loss = 0.10129471123218536\n",
      "2024-09-20T10:20:15Z train loss = 0.16969017684459686\n",
      "2024-09-20T10:20:15Z train loss = 0.2628747820854187\n",
      "2024-09-20T10:20:16Z \n",
      "2024-09-20T10:20:16Z Test set: Average loss: 0.0846, Accuracy: 97.1%\n",
      "2024-09-20T10:20:16Z \n",
      "2024-09-20T10:20:16Z Epoch 4\n",
      "2024-09-20T10:20:16Z train loss = 0.25602805614471436\n",
      "2024-09-20T10:20:16Z train loss = 0.27689531445503235\n",
      "2024-09-20T10:20:16Z train loss = 0.09132082760334015\n",
      "2024-09-20T10:20:16Z train loss = 0.10710184276103973\n",
      "2024-09-20T10:20:16Z \n",
      "2024-09-20T10:20:16Z Test set: Average loss: 0.0693, Accuracy: 97.6%\n",
      "2024-09-20T10:20:16Z \n",
      "2024-09-20T10:20:16Z Epoch 5\n",
      "2024-09-20T10:20:16Z train loss = 0.17908430099487305\n",
      "2024-09-20T10:20:16Z train loss = 0.11153029650449753\n",
      "2024-09-20T10:20:16Z train loss = 0.23699115216732025\n",
      "2024-09-20T10:20:16Z train loss = 0.21319319307804108\n",
      "2024-09-20T10:20:17Z \n",
      "2024-09-20T10:20:17Z Test set: Average loss: 0.0794, Accuracy: 97.3%\n",
      "2024-09-20T10:20:17Z \n",
      "2024-09-20T10:20:17Z Epoch 6\n",
      "2024-09-20T10:20:17Z train loss = 0.03818938508629799\n",
      "2024-09-20T10:20:17Z train loss = 0.17320308089256287\n",
      "2024-09-20T10:20:17Z train loss = 0.09747394919395447\n",
      "2024-09-20T10:20:17Z train loss = 0.2751358151435852\n",
      "2024-09-20T10:20:17Z \n",
      "2024-09-20T10:20:17Z Test set: Average loss: 0.0610, Accuracy: 97.9%\n",
      "2024-09-20T10:20:17Z \n",
      "2024-09-20T10:20:17Z Epoch 7\n",
      "2024-09-20T10:20:17Z train loss = 0.19600754976272583\n",
      "2024-09-20T10:20:17Z train loss = 0.11186692118644714\n",
      "2024-09-20T10:20:17Z train loss = 0.20050616562366486\n",
      "2024-09-20T10:20:18Z train loss = 0.07589271664619446\n",
      "2024-09-20T10:20:18Z \n",
      "2024-09-20T10:20:18Z Test set: Average loss: 0.0600, Accuracy: 97.9%\n",
      "2024-09-20T10:20:18Z \n",
      "2024-09-20T10:20:18Z Epoch 8\n",
      "2024-09-20T10:20:18Z train loss = 0.2577390670776367\n",
      "2024-09-20T10:20:18Z train loss = 0.23042768239974976\n",
      "2024-09-20T10:20:18Z train loss = 0.09219564497470856\n",
      "2024-09-20T10:20:18Z train loss = 0.4270021319389343\n",
      "2024-09-20T10:20:18Z \n",
      "2024-09-20T10:20:18Z Test set: Average loss: 0.0569, Accuracy: 98.1%\n",
      "2024-09-20T10:20:18Z \n",
      "2024-09-20T10:20:18Z Epoch 9\n",
      "2024-09-20T10:20:18Z train loss = 0.319375216960907\n",
      "2024-09-20T10:20:18Z train loss = 0.16024595499038696\n",
      "2024-09-20T10:20:19Z train loss = 0.12301884591579437\n",
      "2024-09-20T10:20:19Z train loss = 0.08181937038898468\n",
      "2024-09-20T10:20:19Z \n",
      "2024-09-20T10:20:19Z Test set: Average loss: 0.0467, Accuracy: 98.5%\n",
      "2024-09-20T10:20:19Z \n",
      "2024-09-20T10:20:20Z [2024-09-20 13:20:20,332] torch.distributed.elastic.agent.server.api: [INFO] [] worker group successfully finished. Waiting 300 seconds for other agents to finish.\n",
      "2024-09-20T10:20:20Z [2024-09-20 13:20:20,332] torch.distributed.elastic.agent.server.api: [INFO] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\n",
      "2024-09-20T10:20:20Z [2024-09-20 13:20:20,334] torch.distributed.elastic.agent.server.api: [INFO] Done waiting for other agents. Elapsed: 0.0016334056854248047 seconds\n"
     ]
    }
   ],
   "source": [
    "job_run.logs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-llm_trainer]",
   "language": "python",
   "name": "conda-env-.mlspace-llm_trainer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
